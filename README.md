# bitnet

![Paper diagram](https://github.com/kevbuh/bitnet/blob/main/paper_img.png)


[1,0,-1] weight transformer

Based on Microsoft's ['The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits'](https://arxiv.org/abs/2402.17764) paper.

Vanilla GPT implementation is from [Andrej Karpathy](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)
