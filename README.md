# bitnet

![Paper diagram](https://github.com/kevbuh/bitnet/blob/main/bitimg.png)

### NO MORE FLOATS!!!

A very simple transformer whose weights are just [1,0,-1]

Based on Microsoft's ['The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits'](https://arxiv.org/abs/2402.17764) paper.




Vanilla GPT implementation is from [Andrej Karpathy](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)

<img src="https://i.kym-cdn.com/entries/icons/facebook/000/035/960/Screen_Shot_2020-12-02_at_3.12.45_PM.jpg" width="400">

This image represents the network well lol
